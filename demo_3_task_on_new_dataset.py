# -*- coding: utf-8 -*-
"""Demo-3 Task on New Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvEDyWfovvuk3kRTzphlvlqVy-KLlSgw
"""

# Install and import necessary packages
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split

# Load CIFAR-10 dataset for this example
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Normalize the pixel values to be between 0 and 1
x_train, x_test = x_train / 255.0, x_test / 255.0

# Convert labels to categorical format
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Display sample images from the dataset
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(x_train[i])
    plt.axis('off')
plt.show()

# Model 1: Basic CNN Model
def create_basic_cnn():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the basic CNN model
model1 = create_basic_cnn()
history1 = model1.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

# Plot training results for model 1
def plot_history(history, model_name):
    plt.plot(history.history['accuracy'], label='accuracy')
    plt.plot(history.history['val_accuracy'], label='val_accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.legend(loc='lower right')
    plt.show()

model1.summary()

plot_history(history1, 'Basic CNN')

# Model 2: Modified CNN with Dropout
def create_cnn_with_dropout():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the model with dropout
model2 = create_cnn_with_dropout()
history2 = model2.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

model2.summary()

# Plot training results for model 2
plot_history(history2, 'CNN with Dropout')

# Model 3: Deeper CNN
def create_deeper_cnn():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the deeper CNN model
model3 = create_deeper_cnn()
history3 = model3.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

model3.summary()

# Plot training results for model 3
plot_history(history3, 'Deeper CNN')

# Model 4: CNN with Batch Normalization
def create_cnn_with_batch_norm():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the model with batch normalization
model4 = create_cnn_with_batch_norm()
history4 = model4.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

model4.summary()


# Plot training results for model 4
plot_history(history4, 'CNN with Batch Normalization')

# Model 5: CNN with Global Average Pooling
def create_cnn_with_global_avg_pool():
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the model with global average pooling
model5 = create_cnn_with_global_avg_pool()
history5 = model5.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

model5.summary()


# Plot training results for model 5
plot_history(history5, 'CNN with Global Average Pooling')

# Model 6: VGG-like CNN
def create_vgg_like_model():
    model = models.Sequential([
        layers.Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(256, (3, 3), activation='relu'),
        layers.Conv2D(256, (3, 3), activation='relu'),
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train the VGG-like model
model6 = create_vgg_like_model()
history6 = model6.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

model6.summary()


# Plot training results for model 6
plot_history(history6, 'VGG-like CNN')

# Summarize results in a table
models = ['Basic CNN', 'CNN with Dropout', 'Deeper CNN', 'CNN with Batch Norm', 'CNN with Global Avg Pool', 'VGG-like CNN']
train_accuracies = [max(history1.history['accuracy']), max(history2.history['accuracy']), max(history3.history['accuracy']),
                    max(history4.history['accuracy']), max(history5.history['accuracy']), max(history6.history['accuracy'])]
val_accuracies = [max(history1.history['val_accuracy']), max(history2.history['val_accuracy']), max(history3.history['val_accuracy']),
                  max(history4.history['val_accuracy']), max(history5.history['val_accuracy']), max(history6.history['val_accuracy'])]
train_losses = [min(history1.history['loss']), min(history2.history['loss']), min(history3.history['loss']),
                min(history4.history['loss']), min(history5.history['loss']), min(history6.history['loss'])]
val_losses = [min(history1.history['val_loss']), min(history2.history['val_loss']), min(history3.history['val_loss']),
              min(history4.history['val_loss']), min(history5.history['val_loss']), min(history6.history['val_loss'])]

import pandas as pd
summary = pd.DataFrame({
    'Model': models,
    'Train Accuracy': train_accuracies,
    'Val Accuracy': val_accuracies,
    'Train Loss': train_losses,
    'Val Loss': val_losses
})

summary

